{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Comparison\n",
    "In previous notebooks, we used a rule-based Information Extraction system to classify documents for TODO?? In this notebook, we'll use **Machine Learning** to classify the documents and then compare the two methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages that we will need\n",
    "from nlp_pneumonia_utils import read_doc_annotations\n",
    "from DocumentClassifier import DocumentClassifier\n",
    "from nlp_pneumonia_utils import list_errors\n",
    "from visual import Vis\n",
    "from visual import snippets_markup\n",
    "from visual import view_pycontext_output\n",
    "from visual import display_doc_text\n",
    "# packages for interaction\n",
    "from IPython.display import display, HTML\n",
    "import ipywidgets\n",
    "\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representing Text Data\n",
    "We need to convert the raw text into a format that can be computed with. To do with, we'll be converting each document into a numerical vector using a **Bag of Words** model.\n",
    "\n",
    "The idea behind a Bag of Words (BOW) model is simple: for each document, we'll jumble together all of the words in the document, not caring about the order they occurred in and represent the documents in a matrix. Each row will represent a document and each column will represent a word in our vocabulary. If a word is present in that document, that column will be 1. If it isn't, that column will be 0.\n",
    "\n",
    "To get an intuition, here's a simple example: Suppose we have these 3 very short lower-cased documents:\n",
    "1. \"the dog ate.\"\n",
    "2. \"the cat sat.\"\n",
    "3. \"the cat sat on the dog.\"\n",
    "\n",
    "In this example, we have a total of 7 words in our vocabulary:\n",
    "\n",
    "V = {the, dog, ate, cat, sat, on, \".\"}\n",
    "\n",
    "To represent this as a vector, here's what our matrix will look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_documents(documents, vectorizer=None, ngrams=(1,1), min_df=1):\n",
    "    if not vectorizer:\n",
    "        vectorizer = CountVectorizer(tokenizer=word_tokenize, ngram_range=ngrams,\n",
    "                                \n",
    "                                     min_df=min_df, lowercase=False)\n",
    "        X = vectorizer.fit_transform(documents)\n",
    "    else:\n",
    "        X = vectorizer.transform(documents)\n",
    "    return X, vectorizer\n",
    "\n",
    "def display_word_matrix(X, vectorizer, n=5):\n",
    "    df = pd.DataFrame(X.todense())\n",
    "    df.columns = vectorizer.get_feature_names()\n",
    "    return df.head(n)\n",
    "\n",
    "#vectorizer_example = CountVectorizer(tokenizer=word_tokenize)\n",
    "example_docs = [\"The dog ate.\", \"The cat sat.\", \"The cat sat on the dog.\"]\n",
    "X_example, vectorizer_example = vectorize_documents(example_docs)\n",
    "display_word_matrix(X_example, vectorizer_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "from collections import Counter\n",
    "def read_in_data():\n",
    "    pos_doc_type='FAM_BREAST_CA_DOC'\n",
    "    docs_train = read_doc_annotations(archive_file='data/bc_train.zip', pos_type=pos_doc_type)\n",
    "\n",
    "    texts_train, labels_train = zip(*((doc.text, \"Positive Document\")\n",
    "                          if doc.annotations[0].type == pos_doc_type\n",
    "                          else (doc.text, \"Negative Document\") \n",
    "                          for doc in docs_train.values()))\n",
    "\n",
    "\n",
    "    docs_test = read_doc_annotations(archive_file='img/bc_test.zip', pos_type=pos_doc_type)\n",
    "    texts_test, labels_test = zip(*((doc.text, \"Positive Document\")\n",
    "                          if doc.annotations[0].type == pos_doc_type\n",
    "                          else (doc.text, \"Negative Document\") \n",
    "                          for doc in docs_test.values()))\n",
    "\n",
    "\n",
    "\n",
    "    # texts += test_texts\n",
    "    # labels += test_labels\n",
    "    c_train = Counter(labels_train)\n",
    "    c_test = Counter(labels_test)\n",
    "    print()\n",
    "    print(\"Number of positive training docs: {}\".format(c_train[\"Positive Document\"]))\n",
    "    print(\"Number of negative training docs: {}\".format(c_train[\"Negative Document\"]))\n",
    "    print()\n",
    "    print(\"Number of positive testing docs: {}\".format(c_test[\"Positive Document\"]))\n",
    "    print(\"Number of negative testing docs: {}\".format(c_test[\"Negative Document\"]))\n",
    "    \n",
    "    return texts_train, labels_train, texts_test, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_train, labels_train, texts_test, labels_test = read_in_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "pos_doc_type='FAM_BREAST_CA_DOC'\n",
    "docs_train = read_doc_annotations(archive_file='data/bc_train.zip', pos_type=pos_doc_type)\n",
    "\n",
    "# I assume doc.positive_label=1 means positive?\n",
    "texts_train, labels_train = zip(*((doc.text, doc.positive_label) for doc in docs_train.values())) \n",
    "\n",
    "# Add in test docs for cross-validation\n",
    "docs_test = read_doc_annotations(archive_file='img/bc_test.zip', pos_type=pos_doc_type)\n",
    "texts_test, labels_test = zip(*((doc.text, doc.positive_label) for doc in docs_test.values()))\n",
    "# texts += test_texts\n",
    "# labels += test_labels\n",
    "print(\"Number of train documents: {}\".format(len(docs_train)))\n",
    "print(\"Number of test documents: {}\".format(len(docs_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our data\n",
    "X_train, vectorizer = vectorize_documents(texts_train)\n",
    "X_test, _ = vectorize_documents(texts_test, vectorizer=vectorizer)\n",
    "display_word_matrix(X_train, vectorizer)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Look at features\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "feature_selector = SelectKBest(chi2, 200)\n",
    "feature_selector.fit_transform(X, labels)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "# (index, score)\n",
    "top_ranked = [(index, score) for (index, score)\n",
    "                in enumerate(feature_selector.scores_)]\n",
    "\n",
    "# Sort by score\n",
    "top_ranked = list(sorted(top_ranked, key=lambda x:x[1], reverse=True))\n",
    "\n",
    "# Use the original index to find the feature name\n",
    "feature_names_and_scores = [(feature_names[idx], score)\n",
    "            for (idx, score) in top_ranked]\n",
    "\n",
    "feature_names_and_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive approach\n",
    "When you Google \"Best machine learning algorithms\", the first result Google suggests is `LogisticRegression`. So, let's try that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "# from sklearn.uti\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, labels_train)\n",
    "pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(labels_test, pred, labels=[\"1\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Better approach\n",
    "Our first attempt got a pretty low score: an F1 of TODO for predicting positive documents. This is much lower than the rule-based system that got an F1 of 0.821 \n",
    "\n",
    "**TODO: Make sure that you're evaluating the same way**\n",
    "\n",
    "But machine learning is rarely an \"out-of-the-box\" kind of task. A first try will rarely do well and there are plenty of tricks to improve our performance. We'll try a few of them right now and see if we can improve our performance.\n",
    "\n",
    "1. **Data Clean-Up** - Looking at our features above, we can see a lot of useless information like punctuation, numbers, and very specific combinations of words that probably don't matter at all for our task. To address this, we'll convert our docuemnts to lower-case, use regular expressions to clean up the text a bit, and set a *document frequency threshold* of 0.2, which will restrict our vocabulary to words that occur in at least 20% of the documents. We'll also expand our features to look at bigrams and trigrams instead of just unigrams (words).\n",
    "2. **Data** - A disadvantage of machine learning is that it typically requires a larger amount of data than manual, rule-based approaches. To maximize the amount of data that we can use, we're going to mix all of our data together and use *5-fold cross-validation* to train and evaluate on each data point, allowing us to use all of our data for both training and testing (importantly, without ever mixing them!)\n",
    "3. **Different Models**: We just picked the first classifier we found on Google, but it's important to try lots of different algorithms and see if one works significantly better than the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's create one dataset for cross-validation.\n",
    "texts = texts_train + texts_test\n",
    "y = labels_train + labels_test\n",
    "print(\"Total number of documents: {}\".format(len(texts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data clean-up\n",
    "import re\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    # Remove punctuation, special symbols\n",
    "    text = re.sub(\"[:./,%#()'\\\"&+-;<>]*\", \"\", text)\n",
    "    # Change any combination digits to be a special NUM symbol\n",
    "    text = re.sub(\"[\\d]+\", \"NUM\", text)\n",
    "    # Remove excess whitespace for human readability\n",
    "    text = re.sub(\"[\\n\\s]+\", \" \", text)\n",
    "    # Add additional code here\n",
    "    ## \n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"****Before clean-up:****\")\n",
    "print(texts[0][:250])\n",
    "print()\n",
    "texts = [preprocess(text) for text in texts]\n",
    "print(\"****After clean-up:****\")\n",
    "print(texts[0][:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform cleaned-up texts with an added document frequency `df`\n",
    "X, vectorizer = vectorize_documents(texts, ngrams=(1,3), min_df=0.2)\n",
    "display_word_matrix(X, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate several classifiers using cross-validation\n",
    "# Compae results\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_clfs(X, y, clfs):\n",
    "    clf_names = []\n",
    "    clf_scores = []\n",
    "    for clf in clfs:\n",
    "        model_name = clf.__repr__().split(\"(\")[0]\n",
    "        pred = cross_val_predict(clf, X, labels, cv=3)\n",
    "        clf_names.append(model_name)\n",
    "        clf_scores.append(f1_score(y, pred))\n",
    "    plot_results(clf_names, clf_scores)\n",
    "    return clf_names, clf_scores\n",
    "\n",
    "def plot_results(clf_names, clf_scores):\n",
    "    x = range(len(clf_names))\n",
    "    plt.plot(x, clf_scores, marker='.')\n",
    "    plt.xticks(x, clf_names, rotation=45)\n",
    "    plt.xlabel(\"Classifier Name\")\n",
    "    plt.ylabel(\"F1\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are the classifiers we'll use\n",
    "clfs = (LogisticRegression(), RandomForestClassifier(random_state=0), \n",
    "        DecisionTreeClassifier(random_state=0), SVC(), MultinomialNB())\n",
    "clf_names, scores = evaluate_clfs(X, y, clfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of our steps clearly worked - The DecisionTreeClassifier got an F1 above 0.9, much higher than both our baseline LogisticRegression model and the rule-based system. Let's look at a more detailed analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = DecisionTreeClassifier()\n",
    "pred = cross_val_predict(dtree, X, y)\n",
    "print(classification_report(y, pred, labels=[1])) # Just look at positive labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps:\n",
    "We tried a few things to improve our machine learning scores. Here are a few more steps we could take:\n",
    "- **Hyperparameter Tuning** - Every machine learning model has hyperparameters that you can adjust. Pick a model and try training it with different hyperparameter combinations until you can find the best score.\n",
    "- **CV** - Try different cross-validation partitions.\n",
    "- **Feature Selection** - Try feature selection methods to reduce the number of features in our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: Visualizing Decision Tree Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain DT classifier using all of the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "dtree = DecisionTreeClassifier(random_state=0)\n",
    "dtree.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_feature_importances(dtree, feature_names,n=10):\n",
    "    importances = sorted(list(zip(feature_names, dtree.feature_importances_)), key=lambda x:x[1], reverse=True)\n",
    "    return importances[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_feature_importances(dtree, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "import collections\n",
    "\n",
    "\n",
    "def visualize_tree(dtree, feature_names):\n",
    "    dot_data = StringIO()\n",
    "    export_graphviz(dtree, out_file=dot_data,  \n",
    "                    filled=True, rounded=True,\n",
    "                    feature_names=feature_names,\n",
    "                    special_characters=True)\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data.getvalue()) \n",
    "    return Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_tree(dtree, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation\n",
    "- Can you make sense of some of these rules?\n",
    "- If not, do you think that's okay?\n",
    "One disadvantage of machine learning is the \"black box\" where it is very difficult for a human to interpret/trust the results of an algorithm. It's important to work past this by going through the results like we did above. One next step you could take is to use the `display_doc_text` from the first notebook to look through some examples of documents and see if these rules make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_doc_map = read_doc_annotations(archive_file='data/bc_train.zip', pos_type=pos_doc_type)\n",
    "pos_docs=dict((k, v) for k, v in annotated_doc_map.items() if  v.annotations[0].type ==pos_doc_type)\n",
    "neg_docs=dict((k, v) for k, v in annotated_doc_map.items() if  v.annotations[0].type !=pos_doc_type)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_doc_text(pos_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Machine Learning offers alternatives to rule-based approaches for certain tasks like document classification. Both have advantages and disadvantages.\n",
    "\n",
    "TODO: Add to this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
