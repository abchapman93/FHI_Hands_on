{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Comparison\n",
    "In previous notebooks, we used a rule-based Information Extraction system to classify documents for family history of cancer. Our two classes are:\n",
    "- **Positive document** - has evidence that a family member has had cancer. \n",
    "- **Negative document** - does not have evidence that family member has had cancer\n",
    "\n",
    "With rule-based NLP systems, you can easily model a concept and write rules to capture it. However, there are certain disadvantages to a rule-based systems\n",
    "\n",
    "### Discussion\n",
    "* What are some disadvantages to rule-based NLP systems?\n",
    "\n",
    "In this notebook, we'll use **Machine Learning** to classify the documents as either **positive** or **negative** and then compare the two methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages that we will need\n",
    "from nlp_pneumonia_utils import read_doc_annotations\n",
    "from DocumentClassifier import DocumentClassifier\n",
    "from nlp_pneumonia_utils import list_errors\n",
    "from visual import Vis\n",
    "from visual import snippets_markup\n",
    "from visual import view_pycontext_output\n",
    "from visual import display_doc_text\n",
    "# packages for interaction\n",
    "from IPython.display import display, HTML\n",
    "import ipywidgets\n",
    "\n",
    "import sklearn\n",
    "\n",
    "# Helper functions\n",
    "from ml_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representing Text Data\n",
    "We need to convert the raw text into a format that can be computed with. To do with, we'll be converting each document into a numerical vector using a **Bag of Words** model.\n",
    "\n",
    "The idea behind a Bag of Words (BOW) model is simple: for each document, we'll jumble together all of the words in the document, not caring about the order they occurred in and represent the documents in a matrix. Each row will represent a document and each column will represent a word in our vocabulary. If a word is present in that document, that column will be 1. If it isn't, that column will be 0.\n",
    "\n",
    "To get an intuition, here's a simple example: Suppose we have these 3 very short lower-cased documents:\n",
    "1. \"the dog ate.\"\n",
    "2. \"the cat sat.\"\n",
    "3. \"the cat sat on the dog.\"\n",
    "\n",
    "In this example, we have a total of 7 words in our vocabulary:\n",
    "\n",
    "V = {the, dog, ate, cat, sat, on, \".\"}\n",
    "\n",
    "To represent this as a vector, here's what our matrix will look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_docs = [\"The dog ate.\", \"The cat sat.\", \"The cat sat on the dog.\"]\n",
    "X_example, vectorizer_example = vectorize_documents(example_docs)\n",
    "display_word_matrix(X_example, vectorizer_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_train, labels_train, texts_test, labels_test = read_in_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform Data\n",
    "X_train, vectorizer = vectorize_documents(texts_train)\n",
    "X_test, _ = vectorize_documents(texts_test, vectorizer=vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at what features we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_word_matrix(X_train, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive approach\n",
    "When you Google \"Best machine learning algorithms\", the first result Google suggests is `LogisticRegression`. So, let's try that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "# from sklearn.uti\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, labels_train)\n",
    "pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(labels_test, pred, labels=[\"Positive Document\"])) # Just look at scores for positive docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Better approach\n",
    "Our first attempt got a pretty low score for predicting positive documents. This is much lower than the rule-based system , which got an F1 of 0.821 \n",
    "\n",
    "But machine learning is rarely an \"out-of-the-box\" kind of task. A first try will rarely do well and there are plenty of tricks to improve our performance. We'll try a few of them right now and see if we can improve our performance.\n",
    "\n",
    "1. **Data Clean-Up** - Looking at our features above, we can see a lot of useless information like punctuation, numbers, and very specific combinations of words that probably don't matter at all for our task. To address this, we'll convert our documents to lower-case, use regular expressions to clean up the text a bit.\n",
    "2. **Features** - We'll first set a *document frequency threshold* of 0.2, which will restrict our vocabulary to words that occur in at least 20% of the documents. We'll also expand our features to look at bigrams and trigrams instead of just unigrams (words).\n",
    "3. **Data** - A disadvantage of machine learning is that it typically requires a larger amount of data. To maximize the amount of data that we can use, we're going to mix all of our data together and use *5-fold cross-validation* to train and evaluate on each data point, allowing us to use all of our data for both training and testing (importantly, without ever mixing them!)\n",
    "4. **Different Models**: We just picked the first classifier we found on Google, but it's important to try lots of different algorithms and see if one works significantly better than the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data clean-up\n",
    "import re\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    # Remove punctuation, special symbols\n",
    "    ## Your code here\n",
    "    \n",
    "    # Change any combination digits to be a special NUM symbol\n",
    "    ## Your code here\n",
    "    \n",
    "    # Remove excess whitespace for human readability\n",
    "    ## Your code here\n",
    "    \n",
    "    # Anything else to try?\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Cross-validation\n",
    "# Create one dataset for cross-validation.\n",
    "texts = texts_train + texts_test\n",
    "y = labels_train + labels_test\n",
    "print(\"Total number of documents: {}\".format(len(texts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"****Before clean-up:****\")\n",
    "print(texts[0][:250])\n",
    "print()\n",
    "texts = [preprocess(text) for text in texts]\n",
    "print(\"****After clean-up:****\")\n",
    "print(texts[0][:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform cleaned-up texts with an added document frequency `df` and larger ngrams\n",
    "X, vectorizer = vectorize_documents(texts, ngrams=(1,3), min_df=0.2)\n",
    "display_word_matrix(X, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are the classifiers we'll use\n",
    "clfs = [LogisticRegression(), RandomForestClassifier(random_state=0), \n",
    "        DecisionTreeClassifier(random_state=0), SVC(), MultinomialNB()]\n",
    "clf_names, scores = evaluate_cross_val_clfs(X, y, clfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of our steps clearly worked - The DecisionTreeClassifier got an F1 above 0.9, much higher than both our baseline LogisticRegression model and the rule-based system. Let's look at a more detailed analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = DecisionTreeClassifier()\n",
    "pred = cross_val_predict(dtree, X, y, cv=5)\n",
    "print(classification_report(y, pred, labels=['Positive Document'])) # Just look at positive labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What To Do Next:\n",
    "We tried a few things to improve our machine learning scores. Here are a few more steps we could take:\n",
    "- **Additional Data Clean-Up** - Remove stopwords, stem words, etc...\n",
    "- **Hyperparameter Tuning** - Every machine learning model has hyperparameters that you can adjust. Pick a model and try training it with different hyperparameter combinations until you can find the best score.\n",
    "- **CV** - Try different cross-validation partitions.\n",
    "- **Feature Selection** - Try feature selection methods to reduce the number of features in our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting Results\n",
    "One advantage of rule-based systems is that the decision-making process *makes sense*. A domain expert can look at a finding, look at a rule, and tell you whether or not it's correct. \n",
    "\n",
    "This is much more challenging with machine learning, which is often viewed as a *black box* that doesn't necessarily make sense to a human. It's important to take a look at results and confirm that they make sense and to identify any potential problems.\n",
    "\n",
    "Our highest performing algorithm is a Decision Tree, which is one of the more easy-to-understand algorithms. `sklearn` has utilities that allow you to visualize the classification process.\n",
    "\n",
    "### Discussion \n",
    "- Look at the decision tree below and trace through the classification process.\n",
    "- Do these rules make sense? If not, why is the classifier still performing so well? \n",
    "- What are the potential problems of this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain DT classifier using all of the data\n",
    "dtree = DecisionTreeClassifier(random_state=0)\n",
    "dtree.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of vocabulary terms\n",
    "feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_tree(dtree, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the document viewer below to look at some examples of positive and negative documents and see if the decision tree's rules apply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_doc_type='FAM_BREAST_CA_DOC'\n",
    "annotated_doc_map = read_doc_annotations(archive_file='data/bc_train.zip', pos_type=pos_doc_type)\n",
    "pos_docs=dict((k, v) for k, v in annotated_doc_map.items() if  v.annotations[0].type ==pos_doc_type)\n",
    "neg_docs=dict((k, v) for k, v in annotated_doc_map.items() if  v.annotations[0].type !=pos_doc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_doc_text(pos_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_doc_text(neg_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
